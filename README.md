# LAB_TASK_7_DEEP_LEARNING

ðŸ“˜ UCS761 â€“ Lab 7
From Numbers to Vision: Building, Breaking, Comparing

Name: Piyush Taneja
Roll No: 102303307
Course: UCS761

ðŸ” Overview

This lab focuses on implementing neural networks completely from scratch using NumPy, without using:

PyTorch

TensorFlow

Keras

Scikit-learn neural network modules

Automatic differentiation tools

The goal of this assignment is not just to make models run, but to deeply understand:

Representation learning

Effect of depth

Activation behavior

Optimization strategies

Gradient stability

Dense vs Convolutional scaling

Generalization

ðŸ§  Part 1 â€“ Deep Networks on Numeric Data
Dataset

Synthetic dataset generated as:

ð‘¥
1
âˆ¼
ð‘ˆ
(
âˆ’
2
,
2
)
x
1
	â€‹

âˆ¼U(âˆ’2,2)
ð‘¥
2
âˆ¼
ð‘ˆ
(
âˆ’
2
,
2
)
x
2
	â€‹

âˆ¼U(âˆ’2,2)

Target:

ð‘¦
=
{
1
	
if 
ð‘¥
1
2
+
ð‘¥
2
2
>
1.5


0
	
otherwise
y={
1
0
	â€‹

if x
1
2
	â€‹

+x
2
2
	â€‹

>1.5
otherwise
	â€‹


Data split:

70% Training

15% Validation

15% Test

Architectures Built

2-layer network (2â€“8â€“1)

5-layer network (2â€“8â€“8â€“8â€“8â€“1)

10-layer network (2â€“8â€“8â€“8â€“8â€“8â€“8â€“8â€“8â€“1)

Hidden activations tested:

ReLU

Sigmoid

Optimizers tested:

SGD

Momentum

Adam

Key Observations

Increasing depth does not always improve validation performance.

Sigmoid networks suffer from vanishing gradients in deeper architectures.

ReLU maintains gradient flow better in deep networks.

Adam converges significantly faster than SGD and Momentum.

ðŸ–¼ Part 2 â€“ Dense vs CNN
Dataset

Synthetic 8Ã—8 image dataset:

Class 0 â†’ Vertical center line

Class 1 â†’ Horizontal center line

Added Gaussian noise (Ïƒ = 0.1)

Models Compared

Dense Network (Flattened 64 inputs)

CNN (3Ã—3 convolution + ReLU + Flatten + Dense)

Parameter Comparison
Model	Parameters
2-layer Dense	33
5-layer Dense	249
10-layer Dense	537
CNN	47
Key Observations

CNN achieves higher test accuracy with fewer parameters.

Dense networks overfit more easily.

CNN generalizes better due to:

Parameter sharing

Local receptive fields

Spatial feature extraction

âš™ Part 3 â€“ Optimizer Comparison (CNN)

Optimizers compared:

SGD

Momentum

Adam

Convergence Ranking

Adam (Fastest)

Momentum

SGD (Slowest)

Observations

Adam reaches near-zero loss within 1â€“2 epochs.

Momentum accelerates convergence compared to SGD.

SGD is stable but slower.

Optimizer choice significantly affects convergence speed.
